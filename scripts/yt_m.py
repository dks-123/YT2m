import os
import re
import httpx
import paramiko
import json
import time
import random
from urllib.parse import urlparse
from datetime import datetime, timedelta

yt_info_path = "yt_info.txt"
output_dir = "output"
cookies_path = os.path.join(os.getcwd(), "cookies.txt")
API_KEY = os.getenv("YT_API_KEY", "")
if not API_KEY:
    print("âŒ ç’°å¢ƒè®Šæ•¸ YT_API_KEY æœªè¨­ç½®ï¼Œæ”¹ç”¨ HTML è§£æ")

SF_L = os.getenv("SF_L", "")
if not SF_L:
    print("âŒ ç’°å¢ƒè®Šæ•¸ SF_L æœªè¨­ç½®")
    exit(1)

parsed_url = urlparse(SF_L)
SFTP_HOST = parsed_url.hostname
SFTP_PORT = parsed_url.port if parsed_url.port else 22
SFTP_USER = parsed_url.username
SFTP_PASSWORD = parsed_url.password
SFTP_REMOTE_DIR = parsed_url.path if parsed_url.path else "/"

os.makedirs(output_dir, exist_ok=True)

# æ–°å¢ï¼šUser-Agent è¼ªæ›¿æ± 
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0"
]

def get_random_headers():
    """ç”Ÿæˆéš¨æ©Ÿ headers"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none"
    }

def add_random_delay():
    """æ·»åŠ éš¨æ©Ÿå»¶é²ï¼Œé¿å…è¢«åµæ¸¬ç‚ºæ©Ÿå™¨äºº"""
    delay = random.uniform(1, 3)
    time.sleep(delay)

def get_channel_id(youtube_url):
    """å¾ YouTube URL æå–é »é“ IDï¼Œå„ªå…ˆä½¿ç”¨ API"""
    handle = youtube_url.split("/")[-2] if "/@" in youtube_url else None
    if API_KEY and handle:
        try:
            url = f"https://www.googleapis.com/youtube/v3/channels?part=id&forHandle={handle}&key={API_KEY}"
            with httpx.Client(timeout=15) as client:
                res = client.get(url)
                res.raise_for_status()
                data = res.json()
                if data.get("items"):
                    print(f"âœ… API æ‰¾åˆ°é »é“ ID: {data['items'][0]['id']}")
                    return data["items"][0]["id"]
                print(f"âš ï¸ API ç„¡æ³•æ‰¾åˆ° {handle} çš„é »é“ IDï¼Œå˜—è©¦ HTML è§£æ")
        except Exception as e:
            print(f"âš ï¸ API ç²å–é »é“ ID å¤±æ•—: {e}")

    # å›é€€åˆ° HTML è§£æ
    try:
        with httpx.Client(http2=True, follow_redirects=True, timeout=15) as client:
            headers = get_random_headers()
            add_random_delay()
            res = client.get(youtube_url, headers=headers)
            html = res.text
            patterns = [
                r'"channelId":"(UC[^"]+)"',
                r'<meta itemprop="channelId" content="(UC[^"]+)"',
                r'"externalId":"(UC[^"]+)"'
            ]
            for pattern in patterns:
                match = re.search(pattern, html)
                if match:
                    print(f"âœ… HTML æ‰¾åˆ°é »é“ ID: {match.group(1)}")
                    return match.group(1)
            print(f"âš ï¸ ç„¡æ³•å¾ {youtube_url} æå–é »é“ ID")
            return None
    except Exception as e:
        print(f"âš ï¸ HTML æå–é »é“ ID å¤±æ•—: {e}")
        return None

def get_live_video_id(channel_id):
    """ä½¿ç”¨ YouTube Data API ç²å–ç›´æ’­ videoId"""
    try:
        url = f"https://www.googleapis.com/youtube/v3/search?part=snippet&channelId={channel_id}&eventType=live&type=video&key={API_KEY}"
        with httpx.Client(timeout=15) as client:
            res = client.get(url)
            res.raise_for_status()
            data = res.json()
            if data.get("items"):
                video_id = data["items"][0]["id"]["videoId"]
                print(f"âœ… æ‰¾åˆ°ç›´æ’­ videoId: {video_id}")
                return f"https://www.youtube.com/watch?v={video_id}"
            print(f"âš ï¸ é »é“ {channel_id} ç›®å‰ç„¡ç›´æ’­ (API è¿”å›ç©ºçµæœ)")
            return None
    except Exception as e:
        print(f"âš ï¸ API è«‹æ±‚å¤±æ•—: {e}")
        return None

def grab_with_multiple_methods(youtube_url):
    """ä½¿ç”¨å¤šç¨®æ–¹æ³•å˜—è©¦æŠ“å– m3u8"""
    methods = [
        ("ç„¡ Cookie æ–¹å¼", grab_without_cookies),
        ("ä½¿ç”¨ Cookie æ–¹å¼", grab_with_cookies),
        ("API è¼”åŠ©æ–¹å¼", grab_with_api_fallback)
    ]
    
    for method_name, method_func in methods:
        print(f"ğŸ”„ å˜—è©¦ {method_name}")
        try:
            result = method_func(youtube_url)
            if result and "googlevideo.com" in result:
                print(f"âœ… {method_name} æˆåŠŸ")
                return result
            else:
                print(f"âš ï¸ {method_name} å¤±æ•—")
        except Exception as e:
            print(f"âŒ {method_name} ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        add_random_delay()
    
    return "https://raw.githubusercontent.com/jz168k/YT2m/main/assets/no_s.m3u8"

def grab_without_cookies(youtube_url):
    """ä¸ä½¿ç”¨ cookies çš„æ–¹å¼æŠ“å–"""
    with httpx.Client(http2=True, follow_redirects=True, timeout=15) as client:
        headers = get_random_headers()
        
        try:
            res = client.get(youtube_url, headers=headers)
            html = res.text

            if 'noindex' in html:
                print(f"âš ï¸ é »é“ {youtube_url} ç›®å‰æœªé–‹å•Ÿç›´æ’­")
                return None

            # å˜—è©¦å¾ player_response JSON ä¸­æå– m3u8
            player_response_match = re.search(r'ytInitialPlayerResponse\s*=\s*({.*?});', html, re.DOTALL)
            if player_response_match:
                player_response = json.loads(player_response_match.group(1))
                streaming_data = player_response.get("streamingData", {})
                hls_formats = streaming_data.get("hlsManifestUrl", "")
                if hls_formats:
                    print(f"âœ… æ‰¾åˆ° .m3u8 é€£çµ: {hls_formats}")
                    return hls_formats

            # å‚™ç”¨æ­£å‰‡è¡¨é”å¼
            m3u8_matches = re.findall(r'(https://[^"]+\.m3u8[^"]*)', html)
            for url in m3u8_matches:
                if "googlevideo.com" in url:
                    print(f"âœ… æ‰¾åˆ° .m3u8 é€£çµ: {url}")
                    return url

            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ .m3u8 é€£çµ")
        except Exception as e:
            print(f"âš ï¸ æŠ“å–é é¢å¤±æ•—: {e}")

        return None

def grab_with_cookies(youtube_url):
    """ä½¿ç”¨ cookies çš„æ–¹å¼æŠ“å–"""
    with httpx.Client(http2=True, follow_redirects=True, timeout=15) as client:
        headers = get_random_headers()

        cookies = {}
        if os.path.exists(cookies_path):
            try:
                with open(cookies_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if not line.startswith('#') and '\t' in line:
                            parts = line.strip().split('\t')
                            if len(parts) >= 6:
                                cookies[parts[5]] = parts[6]
                print(f"âœ… è¼‰å…¥äº† {len(cookies)} å€‹ cookies")
            except Exception as e:
                print(f"âš ï¸ Cookie è®€å–å¤±æ•—: {e}")

        try:
            res = client.get(youtube_url, headers=headers, cookies=cookies)
            html = res.text

            if 'noindex' in html:
                print(f"âš ï¸ é »é“ {youtube_url} ç›®å‰æœªé–‹å•Ÿç›´æ’­")
                return None

            # å˜—è©¦å¾ player_response JSON ä¸­æå– m3u8
            player_response_match = re.search(r'ytInitialPlayerResponse\s*=\s*({.*?});', html, re.DOTALL)
            if player_response_match:
                player_response = json.loads(player_response_match.group(1))
                streaming_data = player_response.get("streamingData", {})
                hls_formats = streaming_data.get("hlsManifestUrl", "")
                if hls_formats:
                    print(f"âœ… æ‰¾åˆ° .m3u8 é€£çµ: {hls_formats}")
                    return hls_formats

            # å‚™ç”¨æ­£å‰‡è¡¨é”å¼
            m3u8_matches = re.findall(r'(https://[^"]+\.m3u8[^"]*)', html)
            for url in m3u8_matches:
                if "googlevideo.com" in url:
                    print(f"âœ… æ‰¾åˆ° .m3u8 é€£çµ: {url}")
                    return url

            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ .m3u8 é€£çµ")
        except Exception as e:
            print(f"âš ï¸ æŠ“å–é é¢å¤±æ•—: {e}")

        return None

def grab_with_api_fallback(youtube_url):
    """ä½¿ç”¨ API è¼”åŠ©çš„æ–¹å¼"""
    if not API_KEY:
        return None
    
    # æå–é »é“ ID
    channel_id = get_channel_id(youtube_url)
    if not channel_id:
        return None
    
    # ä½¿ç”¨ API ç²å–ç›´æ’­ URL
    live_url = get_live_video_id(channel_id)
    if not live_url:
        return None
    
    # ä½¿ç”¨ç„¡ cookies æ–¹å¼æŠ“å–
    return grab_without_cookies(live_url)

def process_yt_info():
    with open(yt_info_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    i = 1
    for line in lines:
        line = line.strip()
        if line.startswith("~~") or not line:
            continue
        if "|" in line:
            parts = line.split("|")
            channel_name = parts[0].strip() if len(parts) > 0 else f"Channel {i}"
        else:
            youtube_url = line
            print(f"ğŸ” å˜—è©¦è§£æ M3U8: {youtube_url}")

            # ä½¿ç”¨å¤šç¨®æ–¹æ³•å˜—è©¦æŠ“å–
            m3u8_url = grab_with_multiple_methods(youtube_url)
            if m3u8_url is None:
                continue

            m3u8_content = f"#EXTM3U\n#EXT-X-STREAM-INF:BANDWIDTH=1280000\n{m3u8_url}\n"
            output_m3u8 = os.path.join(output_dir, f"y{i:02d}.m3u8")
            with open(output_m3u8, "w", encoding="utf-8") as f:
                f.write(m3u8_content)

            php_content = f"""<?php
header('Location: {m3u8_url}');
?>"""
            output_php = os.path.join(output_dir, f"y{i:02d}.php")
            with open(output_php, "w", encoding="utf-8") as f:
                f.write(php_content)

            print(f"âœ… ç”Ÿæˆ {output_m3u8} å’Œ {output_php}")
            i += 1
            
            # æ·»åŠ å»¶é²é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
            add_random_delay()

def upload_files():
    print("ğŸš€ å•Ÿå‹• SFTP ä¸Šå‚³ç¨‹åº...")
    try:
        transport = paramiko.Transport((SFTP_HOST, SFTP_PORT))
        transport.connect(username=SFTP_USER, password=SFTP_PASSWORD)
        sftp = paramiko.SFTPClient.from_transport(transport)

        print(f"âœ… æˆåŠŸé€£æ¥åˆ° SFTPï¼š{SFTP_HOST}")

        try:
            sftp.chdir(SFTP_REMOTE_DIR)
        except IOError:
            print(f"ğŸ“ é ç«¯ç›®éŒ„ {SFTP_REMOTE_DIR} ä¸å­˜åœ¨ï¼Œæ­£åœ¨å‰µå»º...")
            sftp.mkdir(SFTP_REMOTE_DIR)
            sftp.chdir(SFTP_REMOTE_DIR)

        for file in os.listdir(output_dir):
            local_path = os.path.join(output_dir, file)
            remote_path = os.path.join(SFTP_REMOTE_DIR, file)
            if os.path.isfile(local_path):
                print(f"â¬†ï¸ ä¸Šå‚³ {local_path} â†’ {remote_path}")
                sftp.put(local_path, remote_path)

        sftp.close()
        transport.close()
        print("âœ… SFTP ä¸Šå‚³å®Œæˆï¼")

    except Exception as e:
        print(f"âŒ SFTP ä¸Šå‚³å¤±æ•—: {e}")

if __name__ == "__main__":
    process_yt_info()
    upload_files()
